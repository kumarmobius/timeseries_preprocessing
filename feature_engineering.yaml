name: Time Series Preprocessing v4.1
inputs:
  - {name: cleaned_data, type: Dataset, description: "Cleaned dataset from previous component"}
  - {name: cleaning_metadata, type: Data, description: "Metadata from cleaning component"}
  - {name: target_column, type: String, description: "Target column name to predict"}
  - {name: feature_columns, type: String, description: "Comma-separated feature columns (leave empty to use all except target and date)", optional: true, default: ""}
  - {name: seq_length, type: Integer, description: "Sequence window size for LSTM/RNN", optional: true, default: "10"}
  - {name: test_split, type: Float, description: "Test split ratio (0.0-1.0)", optional: true, default: "0.2"}
  - {name: batch_size, type: Integer, description: "Batch size for DataLoader", optional: true, default: "32"}
  - {name: unique_threshold, type: Float, description: "Drop columns with uniqueness % above this (IDs, emails)", optional: true, default: "0.95"}
  - {name: lag_periods, type: String, description: "Comma-separated lag periods (e.g., 1,2,3,7)", optional: true, default: "1,2,3,7"}
  - {name: rolling_windows, type: String, description: "Comma-separated rolling window sizes (e.g., 7,14,30)", optional: true, default: "7,14"}
  - {name: create_lags, type: String, description: "Enable lag feature creation (true/false)", optional: true, default: "true"}
  - {name: create_rolling_stats, type: String, description: "Enable rolling statistics (true/false)", optional: true, default: "true"}
  - {name: create_diff_features, type: String, description: "Enable difference features (true/false)", optional: true, default: "true"}
outputs:
  - {name: processed_data, type: Dataset, description: "DataWrapper containing train_loader, test_loader, and input_dim"}
  - {name: preprocessor, type: Data, description: "Fitted preprocessor for transforming new data"}
  - {name: engineering_metadata, type: Data, description: "JSON metadata with preprocessing statistics"}
implementation:
  container:
    image: kumar2004/ml-base:v1
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import cloudpickle
        import os
        import pickle
        import sys
        import traceback
        from datetime import datetime
        import pandas as pd
        import numpy as np
        import torch
        from sklearn.preprocessing import MinMaxScaler
        from torch.utils.data import TensorDataset, DataLoader

        def ensure_dir_for(p):
            d = os.path.dirname(p)
            if d and not os.path.exists(d):
                os.makedirs(d, exist_ok=True)

        def parse_comma_separated(s, dtype=int):
            if not s or s.strip() == "":
                return []
            return [dtype(x.strip()) for x in s.split(',') if x.strip()]

        def auto_detect_date_column(df):
            print('[INFO] Auto-detecting date column...')
            
            date_keywords = ['date', 'time', 'timestamp', 'datetime', 'day', 'month', 'year', 
                           'created', 'updated', 'modified', 'released', 'release']
            
            candidate_columns = []
            
            # First pass: Check column names for date-related keywords
            for col in df.columns:
                col_lower = col.lower()
                if any(keyword in col_lower for keyword in date_keywords):
                    candidate_columns.append((col, 100))  # High priority
            
            # Second pass: Try to parse each column as datetime
            for col in df.columns:
                if col in [c[0] for c in candidate_columns]:
                    continue
                
                # Skip numeric columns
                if pd.api.types.is_numeric_dtype(df[col]):
                    continue
                
                # Try parsing a sample of the column
                try:
                    sample = df[col].dropna().head(100)
                    if len(sample) == 0:
                        continue
                    
                    # Try to parse with mixed format
                    parsed = pd.to_datetime(sample, errors='coerce', format='mixed', dayfirst=True)
                    
                    # If more than 80% parsed successfully, it's likely a date column
                    success_rate = parsed.notna().sum() / len(sample)
                    if success_rate > 0.8:
                        candidate_columns.append((col, success_rate * 50))  # Medium priority
                        print(f'[INFO] Found potential date column: {col} (success rate: {success_rate:.2%})')
                
                except Exception as e:
                    continue
            
            if not candidate_columns:
                print('[WARN] No date column detected. Proceeding without temporal ordering.')
                return None
            
            # Sort by priority (highest first) and return the best candidate
            candidate_columns.sort(key=lambda x: x[1], reverse=True)
            detected_col = candidate_columns[0][0]
            
            print(f'[INFO] Selected date column: {detected_col}')
            return detected_col

        def parse_date_column(df, date_col):
            print(f'[INFO] Parsing date column: {date_col}')
            
            original_dtype = df[date_col].dtype
            print(f'[INFO] Original dtype: {original_dtype}')
            
            # If already datetime, return as-is
            if pd.api.types.is_datetime64_any_dtype(df[date_col]):
                print('[INFO] Column is already datetime type')
                return df[date_col]
            
            # Try multiple parsing strategies
            strategies = [
                # Strategy 1: Mixed format with dayfirst=True (for DD-MM-YYYY)
                lambda x: pd.to_datetime(x, errors='coerce', format='mixed', dayfirst=True),
                
                # Strategy 2: Mixed format with dayfirst=False (for MM-DD-YYYY)
                lambda x: pd.to_datetime(x, errors='coerce', format='mixed', dayfirst=False),
                
                # Strategy 3: ISO8601 format
                lambda x: pd.to_datetime(x, errors='coerce', format='ISO8601'),
                
                # Strategy 4: Infer format
                lambda x: pd.to_datetime(x, errors='coerce', infer_datetime_format=True),
            ]
            
            for i, strategy in enumerate(strategies, 1):
                try:
                    parsed = strategy(df[date_col])
                    success_rate = parsed.notna().sum() / len(df[date_col])
                    
                    print(f'[INFO] Strategy {i} success rate: {success_rate:.2%}')
                    
                    if success_rate > 0.95:  # At least 95% success
                        print(f'[INFO] Using strategy {i} (success rate: {success_rate:.2%})')
                        
                        # Show sample of parsed dates
                        sample_original = df[date_col].dropna().head(3).tolist()
                        sample_parsed = parsed.dropna().head(3).tolist()
                        print(f'[INFO] Sample original: {sample_original}')
                        print(f'[INFO] Sample parsed: {sample_parsed}')
                        
                        return parsed
                
                except Exception as e:
                    print(f'[WARN] Strategy {i} failed: {str(e)}')
                    continue
            
            # If all strategies fail, raise error
            raise ValueError(f'Failed to parse date column "{date_col}". Please check the date format.')

        def create_lag_features(df, columns, lag_periods):
            # Filter to numeric columns only
            numeric_cols = [col for col in columns if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
            print('[INFO] Creating lag features for', len(numeric_cols), 'numeric columns with lags:', lag_periods)
            if len(numeric_cols) > 0:
                print('[INFO] Numeric columns:', numeric_cols[:5], '...' if len(numeric_cols) > 5 else '')
            
            if not numeric_cols or not lag_periods:
                return df
            
            # Create all lag features at once using concat for better performance
            lag_features = {}
            for col in numeric_cols:
                for lag in lag_periods:
                    lag_features[col + '_lag_' + str(lag)] = df[col].shift(lag)
            
            if lag_features:
                df = pd.concat([df, pd.DataFrame(lag_features, index=df.index)], axis=1)
            
            return df

        def create_rolling_features(df, columns, rolling_windows):
            # Filter to numeric columns only
            numeric_cols = [col for col in columns if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
            print('[INFO] Creating rolling features for', len(numeric_cols), 'numeric columns with windows:', rolling_windows)
            if len(numeric_cols) > 0:
                print('[INFO] Numeric columns:', numeric_cols[:5], '...' if len(numeric_cols) > 5 else '')
            
            if not numeric_cols or not rolling_windows:
                return df
            
            # Create all rolling features at once using concat for better performance
            rolling_features = {}
            for col in numeric_cols:
                for window in rolling_windows:
                    rolling_features[col + '_rolling_mean_' + str(window)] = df[col].rolling(window).mean()
                    rolling_features[col + '_rolling_std_' + str(window)] = df[col].rolling(window).std()
                    rolling_features[col + '_rolling_min_' + str(window)] = df[col].rolling(window).min()
                    rolling_features[col + '_rolling_max_' + str(window)] = df[col].rolling(window).max()
            
            if rolling_features:
                df = pd.concat([df, pd.DataFrame(rolling_features, index=df.index)], axis=1)
            
            return df

        def create_diff_features(df, columns):
            # Filter to numeric columns only
            numeric_cols = [col for col in columns if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
            print('[INFO] Creating difference features for', len(numeric_cols), 'numeric columns')
            if len(numeric_cols) > 0:
                print('[INFO] Numeric columns:', numeric_cols[:5], '...' if len(numeric_cols) > 5 else '')
            
            if not numeric_cols:
                return df
            
            # Create all diff features at once using concat for better performance
            diff_features = {}
            for col in numeric_cols:
                diff_features[col + '_diff'] = df[col].diff()
                diff_features[col + '_pct_change'] = df[col].pct_change()
            
            if diff_features:
                df = pd.concat([df, pd.DataFrame(diff_features, index=df.index)], axis=1)
            
            return df

        def encode_categorical_safe(df, exclude_cols=None):
            exclude_cols = set(exclude_cols or [])
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns
            categorical_cols = [c for c in categorical_cols if c not in exclude_cols]
            
            encoding_info = {}
            
            for col in categorical_cols:
                nunique = df[col].nunique()
                print('[INFO] Encoding', col, 'with', nunique, 'unique values')
                
                if nunique <= 10:
                    dummies = pd.get_dummies(df[col], prefix=col, drop_first=False)
                    df = pd.concat([df.drop(columns=[col]), dummies], axis=1)
                    encoding_info[col] = {
                        'type': 'onehot',
                        'columns': list(dummies.columns)
                    }
                    print('[INFO] One-hot encoded', col, 'into', len(dummies.columns), 'columns')
                else:
                    freq_map = df[col].value_counts().to_dict()
                    df[col] = df[col].map(freq_map).fillna(0)
                    encoding_info[col] = {
                        'type': 'frequency',
                        'mapping': {str(k): int(v) for k, v in list(freq_map.items())[:10]}  # Save only top 10
                    }
                    print('[INFO] Frequency encoded', col)
            
            return df, encoding_info

        def drop_high_uniqueness_columns(df, feature_cols, unique_threshold):
            print('[INFO] Checking for high-uniqueness columns (threshold:', unique_threshold, ')')
            cols_to_drop = []
            uniqueness_report = {}
            
            for col in feature_cols:
                if col not in df.columns:
                    continue
                unique_pct = df[col].nunique() / len(df)
                uniqueness_report[col] = float(unique_pct)
                
                if unique_pct > unique_threshold:
                    if pd.api.types.is_numeric_dtype(df[col]):
                        std = df[col].std()
                        if std > 0:
                            continue
                    cols_to_drop.append(col)
                    print('[WARN] Dropping', col, '(uniqueness:', round(unique_pct * 100, 2), '%)')
            
            if cols_to_drop:
                df = df.drop(columns=cols_to_drop)
                feature_cols = [c for c in feature_cols if c not in cols_to_drop]
                print('[INFO] Dropped', len(cols_to_drop), 'high-uniqueness columns')
            else:
                print('[INFO] No high-uniqueness columns to drop')
            
            return df, feature_cols, cols_to_drop, uniqueness_report

        def create_sequences(data, labels, seq_length):
            X, y = [], []
            for i in range(len(data) - seq_length):
                X.append(data[i:i+seq_length])
                y.append(labels[i+seq_length])
            return np.array(X), np.array(y)

        class TimeSeriesPreprocessor:
            def __init__(self):
                self.scaler = None
                self.feature_columns = []
                self.target_column = None
                self.date_column = None
                self.seq_length = None
                self.lag_periods = []
                self.rolling_windows = []
                self.categorical_encodings = {}
                self.feature_order = []
                self.create_lags = True
                self.create_rolling_stats = True
                self.create_diff_features = True
                self.unique_threshold = 0.95
                
            def fit(self, df, config):
                self.target_column = config['target_column']
                self.date_column = config.get('date_column')
                self.seq_length = config['seq_length']
                self.lag_periods = config['lag_periods']
                self.rolling_windows = config['rolling_windows']
                self.create_lags = config['create_lags']
                self.create_rolling_stats = config['create_rolling_stats']
                self.create_diff_features = config['create_diff_features']
                self.unique_threshold = config.get('unique_threshold', 0.95)
                
                exclude_cols = [self.target_column]
                if self.date_column:
                    exclude_cols.append(self.date_column)
                
                self.feature_columns = [c for c in df.columns if c not in exclude_cols]
                
                return self
                
            def transform(self, df):
                df = df.copy()
                
                if self.date_column and self.date_column in df.columns:
                    df[self.date_column] = parse_date_column(df, self.date_column)
                    df = df.sort_values(by=self.date_column).reset_index(drop=True)
                
                # Check boolean flags properly (they are stored as booleans, not callables)
                if self.create_lags is True and self.lag_periods:
                    df = create_lag_features(df, self.feature_columns, self.lag_periods)
                
                if self.create_rolling_stats is True and self.rolling_windows:
                    df = create_rolling_features(df, self.feature_columns, self.rolling_windows)
                
                if self.create_diff_features is True:
                    df = create_diff_features(df, self.feature_columns)
                
                df = df.bfill().ffill()
                
                exclude = [self.target_column]
                if self.date_column and self.date_column in df.columns:
                    exclude.append(self.date_column)
                
                df, _ = encode_categorical_safe(df, exclude_cols=exclude)
                
                final_features = [c for c in df.columns if c not in exclude]
                
                df, final_features, _, _ = drop_high_uniqueness_columns(df, final_features, self.unique_threshold)
                
                X = df[final_features].values
                y = df[self.target_column].values
                
                if self.scaler is not None:
                    X_scaled = self.scaler.transform(X)
                else:
                    self.scaler = MinMaxScaler()
                    X_scaled = self.scaler.fit_transform(X)
                
                X_seq, y_seq = create_sequences(X_scaled, y, self.seq_length)
                
                return X_seq, y_seq
            
            def save(self, path):
                ensure_dir_for(path)
                with open(path, 'wb') as f:
                    cloudpickle.dump(self, f)
                                
            @staticmethod
            def load(path):
                with open(path, 'rb') as f:
                    return pickle.load(f)

        class DataWrapper:
            def __init__(self, train_loader, test_loader, input_dim, scaler, feature_columns, metadata):
                self.train_loader = train_loader
                self.test_loader = test_loader
                self.input_dim = input_dim
                self.scaler = scaler
                self.feature_columns = feature_columns
                self.metadata = metadata

        parser = argparse.ArgumentParser()
        parser.add_argument('--cleaned_data', type=str, required=True)
        parser.add_argument('--cleaning_metadata', type=str, required=True)
        parser.add_argument('--target_column', type=str, required=True)
        parser.add_argument('--feature_columns', type=str, default="")
        parser.add_argument('--seq_length', type=int, default=10)
        parser.add_argument('--test_split', type=float, default=0.2)
        parser.add_argument('--batch_size', type=int, default=32)
        parser.add_argument('--unique_threshold', type=float, default=0.95)
        parser.add_argument('--lag_periods', type=str, default="1,2,3,7")
        parser.add_argument('--rolling_windows', type=str, default="7,14")
        parser.add_argument('--create_lags', type=str, default="true")
        parser.add_argument('--create_rolling_stats', type=str, default="true")
        parser.add_argument('--create_diff_features', type=str, default="true")
        parser.add_argument('--processed_data', type=str, required=True)
        parser.add_argument('--preprocessor', type=str, required=True)
        parser.add_argument('--engineering_metadata', type=str, required=True)
        args = parser.parse_args()

        try:
            print("="*80)
            print("TIME SERIES PREPROCESSING v3.1 (AUTO DATE DETECTION)")
            print("="*80)
            
            # FIXED: Renamed variables to avoid shadowing function names
            enable_lags = args.create_lags.lower() in ('true', 't', 'yes', 'y', '1')
            enable_rolling_stats = args.create_rolling_stats.lower() in ('true', 't', 'yes', 'y', '1')
            enable_diff_features = args.create_diff_features.lower() in ('true', 't', 'yes', 'y', '1')
            
            lag_periods = parse_comma_separated(args.lag_periods, int)
            rolling_windows = parse_comma_separated(args.rolling_windows, int)
            
            print('[STEP 1/13] Loading data...')
            df = pd.read_parquet(args.cleaned_data)
            print('[INFO] Loaded data shape:', df.shape)
            print('[INFO] Columns:', list(df.columns))
            
            with open(args.cleaning_metadata, 'r') as f:
                cleaning_meta = json.load(f)
            
            print('[STEP 2/13] Validating target column...')
            if args.target_column not in df.columns:
                raise ValueError('Target column not found: ' + args.target_column)
            print('[INFO] Target column:', args.target_column)
            
            print('[STEP 3/13] Auto-detecting date column...')
            date_column = auto_detect_date_column(df)
            
            if date_column:
                print('[INFO] Detected date column:', date_column)
                
                print('[STEP 4/13] Parsing and sorting by date (CRITICAL for time series)...')
                df[date_column] = parse_date_column(df, date_column)
                df = df.sort_values(by=date_column).reset_index(drop=True)
                print('[INFO] Date range:', df[date_column].min(), 'to', df[date_column].max())
            else:
                print('[WARN] No date column detected - proceeding without temporal ordering')
                print('[WARN] This may impact time series model performance')
            
            print('[STEP 5/13] Determining feature columns...')
            if args.feature_columns.strip():
                feature_cols = parse_comma_separated(args.feature_columns, str)
                missing = [c for c in feature_cols if c not in df.columns]
                if missing:
                    raise ValueError('Feature columns not found: ' + str(missing))
            else:
                exclude = [args.target_column]
                if date_column:
                    exclude.append(date_column)
                feature_cols = [c for c in df.columns if c not in exclude]
            
            print('[INFO] Feature columns:', len(feature_cols))
            print('[INFO] Features:', feature_cols[:5], '...' if len(feature_cols) > 5 else '')
            
            print('[STEP 6/13] Creating time series features...')
            initial_shape = df.shape
            
            # FIXED: Using renamed variables instead of function names
            if enable_lags and lag_periods:
                df = create_lag_features(df, feature_cols, lag_periods)
            
            if enable_rolling_stats and rolling_windows:
                df = create_rolling_features(df, feature_cols, rolling_windows)
            
            if enable_diff_features:
                df = create_diff_features(df, feature_cols)
            
            print('[INFO] Shape after feature engineering:', df.shape)
            print('[INFO] Created', df.shape[1] - initial_shape[1], 'new features')
            
            print('[STEP 7/13] Handling missing values and infinity values...')
            nan_before = df.isna().sum().sum()
            
            # Replace infinity values with NaN
            df = df.replace([np.inf, -np.inf], np.nan)
            inf_count = df.isna().sum().sum() - nan_before
            if inf_count > 0:
                print('[INFO] Replaced', inf_count, 'infinity values with NaN')
            
            # Backward-fill then forward-fill
            df = df.bfill()
            df = df.ffill()
            df = df.dropna()
            nan_after = df.isna().sum().sum()
            print('[INFO] NaN values:', nan_before, '->', nan_after)
            print('[INFO] Shape after cleaning:', df.shape)
            
            print('[STEP 8/13] Encoding categorical features...')
            exclude_cols = [args.target_column]
            if date_column:
                exclude_cols.append(date_column)
            
            df, encoding_info = encode_categorical_safe(df, exclude_cols=exclude_cols)
            print('[INFO] Shape after encoding:', df.shape)
            
            print('[STEP 9/13] Dropping high-uniqueness columns (IDs, emails, etc.)...')
            final_feature_cols = [c for c in df.columns 
                                 if c not in ([args.target_column] + ([date_column] if date_column else []))]
            
            df, final_feature_cols, dropped_unique_cols, uniqueness_report = drop_high_uniqueness_columns(
                df, final_feature_cols, args.unique_threshold
            )
            print('[INFO] Shape after dropping unique columns:', df.shape)
            
            print('[STEP 10/13] Preparing features and target...')
            X = df[final_feature_cols].values
            y = df[args.target_column].values
            print('[INFO] Features shape:', X.shape)
            print('[INFO] Target shape:', y.shape)
            
            print('[STEP 11/13] Scaling features...')
            scaler = MinMaxScaler()
            X_scaled = scaler.fit_transform(X)
            print('[INFO] Scaling complete')
            
            print('[STEP 12/13] Creating sequences (window size=' + str(args.seq_length) + ')...')
            X_seq, y_seq = create_sequences(X_scaled, y, args.seq_length)
            print('[INFO] Sequence shape:', X_seq.shape)
            print('[INFO] Labels shape:', y_seq.shape)
            
            print('[STEP 13/13] Splitting into train/test (chronological, no shuffle)...')
            train_size = int(len(X_seq) * (1 - args.test_split))
            
            X_train = X_seq[:train_size]
            y_train = y_seq[:train_size]
            X_test = X_seq[train_size:]
            y_test = y_seq[train_size:]
            
            print('[INFO] Train shape:', X_train.shape, 'Test shape:', X_test.shape)
            print('[INFO] Train samples:', len(X_train), 'Test samples:', len(X_test))
            
            X_train = torch.tensor(X_train, dtype=torch.float32)
            y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)
            X_test = torch.tensor(X_test, dtype=torch.float32)
            y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)
            
            train_dataset = TensorDataset(X_train, y_train)
            test_dataset = TensorDataset(X_test, y_test)
            
            train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False)
            test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)
            
            input_dim = X_train.shape[2]
            print('[INFO] Input dimension (features):', input_dim)
            
            print('[STEP 14/14] Saving outputs...')
            
            data_wrapper = DataWrapper(
                train_loader=train_loader,
                test_loader=test_loader,
                input_dim=input_dim,
                scaler=scaler,
                feature_columns=final_feature_cols,
                metadata={
                    'seq_length': args.seq_length,
                    'target_column': args.target_column,
                    'date_column': date_column,
                    'train_samples': len(X_train),
                    'test_samples': len(X_test),
                    'train_shape': list(X_train.shape),
                    'test_shape': list(X_test.shape),
                    'batch_size': args.batch_size,
                    'test_split': args.test_split
                }
            )
            
            ensure_dir_for(args.processed_data)
            with open(args.processed_data, 'wb') as f:
                cloudpickle.dump(data_wrapper, f)
            print('[INFO] Saved processed_data to:', args.processed_data)
            
            preprocessor = TimeSeriesPreprocessor()
            preprocessor.fit(df, {
                'target_column': args.target_column,
                'date_column': date_column,
                'seq_length': args.seq_length,
                'lag_periods': lag_periods,
                'rolling_windows': rolling_windows,
                'create_lags': enable_lags,
                'create_rolling_stats': enable_rolling_stats,
                'create_diff_features': enable_diff_features,
                'unique_threshold': args.unique_threshold
            })
            preprocessor.scaler = scaler
            preprocessor.categorical_encodings = encoding_info
            preprocessor.feature_order = final_feature_cols
            
            preprocessor.save(args.preprocessor)
            print('[INFO] Saved preprocessor to:', args.preprocessor)
            
            print('[STEP 15/15] Saving metadata...')
            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'date_column': date_column,
                'date_column_auto_detected': True,
                'target_column': args.target_column,
                'feature_columns': final_feature_cols,
                'n_features': len(final_feature_cols),
                'seq_length': args.seq_length,
                'test_split': args.test_split,
                'batch_size': args.batch_size,
                'unique_threshold': args.unique_threshold,
                'lag_periods': lag_periods,
                'rolling_windows': rolling_windows,
                'create_lags': enable_lags,
                'create_rolling_stats': enable_rolling_stats,
                'create_diff_features': enable_diff_features,
                'shapes': {
                    'X_train': list(X_train.shape),
                    'y_train': list(y_train.shape),
                    'X_test': list(X_test.shape),
                    'y_test': list(y_test.shape)
                },
                'input_dim': input_dim,
                'date_range': {
                    'start': str(df[date_column].min()) if date_column else None,
                    'end': str(df[date_column].max()) if date_column else None
                },
                'initial_shape': list(initial_shape),
                'final_shape': list(df.shape),
                'rows_dropped_for_sequences': args.seq_length,
                'dropped_unique_columns': dropped_unique_cols,
                'uniqueness_report': uniqueness_report,
                'features_created': {
                    'lag_features': len(lag_periods) * len(feature_cols) if enable_lags else 0,
                    'rolling_features': len(rolling_windows) * 4 * len(feature_cols) if enable_rolling_stats else 0,
                    'diff_features': 2 * len(feature_cols) if enable_diff_features else 0
                },
                'categorical_encodings': encoding_info,
                'cleaning_metadata': cleaning_meta
            }
            
            ensure_dir_for(args.engineering_metadata)
            with open(args.engineering_metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            print('[INFO] Saved metadata to:', args.engineering_metadata)
            
            print("="*80)
            print("TIME SERIES PREPROCESSING COMPLETE")
            print("="*80)
            print('Input shape:', initial_shape)
            print('Final shape:', df.shape)
            print('Train sequences:', X_train.shape)
            print('Test sequences:', X_test.shape)
            print('Input dimension:', input_dim)
            print('Sequence length:', args.seq_length)
            print('Batch size:', args.batch_size)
            print('Target column:', args.target_column)
            print('Date column (auto-detected):', date_column if date_column else 'None')
            print('Dropped unique columns:', len(dropped_unique_cols))
            print("="*80)
            print("SUCCESS: All outputs saved")
            print("="*80)
            
        except Exception as exc:
            print('ERROR:', str(exc), file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --cleaned_data
      - {inputPath: cleaned_data}
      - --cleaning_metadata
      - {inputPath: cleaning_metadata}
      - --target_column
      - {inputValue: target_column}
      - --feature_columns
      - {inputValue: feature_columns}
      - --seq_length
      - {inputValue: seq_length}
      - --test_split
      - {inputValue: test_split}
      - --batch_size
      - {inputValue: batch_size}
      - --unique_threshold
      - {inputValue: unique_threshold}
      - --lag_periods
      - {inputValue: lag_periods}
      - --rolling_windows
      - {inputValue: rolling_windows}
      - --create_lags
      - {inputValue: create_lags}
      - --create_rolling_stats
      - {inputValue: create_rolling_stats}
      - --create_diff_features
      - {inputValue: create_diff_features}
      - --processed_data
      - {outputPath: processed_data}
      - --preprocessor
      - {outputPath: preprocessor}
      - --engineering_metadata
      - {outputPath: engineering_metadata}
