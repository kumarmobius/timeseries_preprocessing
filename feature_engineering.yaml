name: Time Series Preprocessing v5.0
description: Preprocesses time series data with auto date/epoch detection and robust inference support for LSTM/RNN models
inputs:
  - {name: cleaned_data, type: Dataset}
  - {name: cleaning_metadata, type: Data}
  - {name: target_column, type: String}
  - {name: feature_columns, type: String, optional: true, default: ""}
  - {name: seq_length, type: Integer, optional: true, default: "10"}
  - {name: test_split, type: Float, optional: true, default: "0.2"}
  - {name: batch_size, type: Integer, optional: true, default: "32"}
  - {name: unique_threshold, type: Float, optional: true, default: "0.95"}
  - {name: lag_periods, type: String, optional: true, default: "1,2,3,7"}
  - {name: rolling_windows, type: String, optional: true, default: "7,14"}
  - {name: create_lags, type: String, optional: true, default: "true"}
  - {name: create_rolling_stats, type: String, optional: true, default: "true"}
  - {name: create_diff_features, type: String, optional: true, default: "true"}
outputs:
  - {name: processed_data, type: Dataset}
  - {name: preprocessor, type: Data}
  - {name: engineering_metadata, type: Data}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, json, cloudpickle, os, sys, traceback
        from datetime import datetime
        import pandas as pd, numpy as np, torch
        from sklearn.preprocessing import MinMaxScaler
        from torch.utils.data import TensorDataset, DataLoader

        def ensure_dir(p):
            d=os.path.dirname(p)
            if d: os.makedirs(d,exist_ok=True)

        def parse_csv(s,dtype=int):
            return [dtype(x.strip()) for x in s.split(',') if x.strip()] if s.strip() else []

        def auto_detect_date(df):
            kw=['date','time','timestamp','datetime','day','month','year','created','updated','modified']
            cands=[]
            for col in df.columns:
                if any(k in col.lower() for k in kw): cands.append((col,100))
            for col in df.columns:
                if col in [c[0] for c in cands] or pd.api.types.is_numeric_dtype(df[col]): continue
                try:
                    s=df[col].dropna().head(100)
                    if len(s)==0: continue
                    p=pd.to_datetime(s,errors='coerce',format='mixed',dayfirst=True)
                    r=p.notna().sum()/len(s)
                    if r>0.8: cands.append((col,r*50))
                except: pass
            return cands[0][0] if cands else None

        def detect_timestamp(df):
            ranges={'seconds':(946684800,2524608000),'milliseconds':(946684800000,2524608000000),'microseconds':(946684800000000,2524608000000000)}
            kw=['timestamp','epoch','time','unix','ts','created_at','updated_at']
            cands=[]
            for col in df.select_dtypes(include=[np.number]).columns:
                nm=any(k in col.lower() for k in kw)
                s=df[col].dropna()
                if len(s)==0: continue
                mn,mx=s.min(),s.max()
                for u,(rn,rx) in ranges.items():
                    if rn<=mn and mx<=rx:
                        cands.append({'col':col,'unit':u,'pr':100 if nm else 50})
                        break
            if not cands: return None,None
            c=max(cands,key=lambda x:x['pr'])
            col,u=c['col'],c['unit']
            try:
                nc=col+'_datetime'
                df[nc]=pd.to_datetime(df[col],unit='s' if u=='seconds' else 'ms' if u=='milliseconds' else 'us')
                return (nc,col) if df[nc].notna().sum()>len(df)*0.95 else (None,None)
            except: return None,None

        def parse_date(df,col):
            if pd.api.types.is_datetime64_any_dtype(df[col]): return df[col]
            for st in [lambda x:pd.to_datetime(x,errors='coerce',format='mixed',dayfirst=True),
                       lambda x:pd.to_datetime(x,errors='coerce',format='mixed',dayfirst=False),
                       lambda x:pd.to_datetime(x,errors='coerce',format='ISO8601'),
                       lambda x:pd.to_datetime(x,errors='coerce',infer_datetime_format=True)]:
                try:
                    p=st(df[col])
                    if p.notna().sum()/len(df[col])>0.95: return p
                except: pass
            raise ValueError('Cannot parse date:'+col)

        def lag_feat(df,cols,lags):
            nc=[c for c in cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]
            if not nc or not lags: return df
            return pd.concat([df,pd.DataFrame({c+'_lag_'+str(l):df[c].shift(l) for c in nc for l in lags},index=df.index)],axis=1)

        def roll_feat(df,cols,wins):
            nc=[c for c in cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]
            if not nc or not wins: return df
            rf={}
            for c in nc:
                for w in wins:
                    rf[c+'_rmean_'+str(w)]=df[c].rolling(w).mean()
                    rf[c+'_rstd_'+str(w)]=df[c].rolling(w).std()
                    rf[c+'_rmin_'+str(w)]=df[c].rolling(w).min()
                    rf[c+'_rmax_'+str(w)]=df[c].rolling(w).max()
            return pd.concat([df,pd.DataFrame(rf,index=df.index)],axis=1)

        def diff_feat(df,cols):
            nc=[c for c in cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]
            if not nc: return df
            return pd.concat([df,pd.DataFrame({c+'_diff':df[c].diff(),c+'_pct':df[c].pct_change() for c in nc},index=df.index)],axis=1)

        def enc_cat_tr(df,ex):
            ex=set(ex or [])
            cc=[c for c in df.select_dtypes(include=['object','category']).columns if c not in ex]
            ei={}
            for c in cc:
                nu=df[c].nunique()
                if nu<=10:
                    d=pd.get_dummies(df[c],prefix=c,drop_first=False)
                    df=pd.concat([df.drop(columns=[c]),d],axis=1)
                    ei[c]={'type':'oh','cols':list(d.columns)}
                else:
                    fm=df[c].value_counts().to_dict()
                    df[c]=df[c].map(fm).fillna(0)
                    ei[c]={'type':'freq','map':fm}
            return df,ei

        def enc_cat_inf(df,ei,ex):
            ex=set(ex or [])
            for c,inf in ei.items():
                if c not in df.columns or c in ex: continue
                if inf['type']=='oh':
                    ec=inf['cols']
                    d=pd.get_dummies(df[c],prefix=c,drop_first=False)
                    for e in ec:
                        if e not in d.columns: d[e]=0
                    df=pd.concat([df.drop(columns=[c]),d[ec]],axis=1)
                elif inf['type']=='freq':
                    df[c]=df[c].map(inf['map']).fillna(0)
            return df

        def drop_uniq(df,fc,th):
            cd=[]
            for c in fc:
                if c not in df.columns: continue
                up=df[c].nunique()/len(df)
                if up>th:
                    if pd.api.types.is_numeric_dtype(df[c]) and df[c].std()>0: continue
                    cd.append(c)
            if cd: df=df.drop(columns=cd)
            return df,[c for c in fc if c not in cd],cd

        def seq(d,l,sl):
            X,y=[],[]
            for i in range(len(d)-sl):
                X.append(d[i:i+sl])
                y.append(l[i+sl])
            return np.array(X),np.array(y)

        class TSP:
            def __init__(self):
                self.sc=None;self.fc=[];self.tc=None;self.dc=None;self.oc=None;self.sl=None;self.lp=[];self.rw=[]
                self.ce={};self.fo=[];self.cl=True;self.cr=True;self.cd=True;self.ut=0.95;self.du=[]
            def fit(self,df,cfg):
                self.tc=cfg['tc'];self.dc=cfg.get('dc');self.oc=cfg.get('oc');self.sl=cfg['sl'];self.lp=cfg['lp']
                self.rw=cfg['rw'];self.cl=cfg['cl'];self.cr=cfg['cr'];self.cd=cfg['cd'];self.ut=cfg.get('ut',0.95)
                ex=[self.tc]
                if self.dc: ex.append(self.dc)
                if self.oc: ex.append(self.oc)
                self.fc=[c for c in df.columns if c not in ex]
                return self
            def tr(self,df,tm=False):
                df=df.copy()
                ddc,dtc=self.dc,None
                if not ddc or ddc not in df.columns:
                    ddc=auto_detect_date(df)
                if not ddc:
                    ddc,dtc=detect_timestamp(df)
                if not ddc:
                    raise ValueError('No temporal column')
                if ddc!=self.dc:
                    self.dc=ddc
                    if dtc: self.oc=dtc
                df[self.dc]=parse_date(df,self.dc)
                df=df.sort_values(by=self.dc).reset_index(drop=True)
                if self.cl and self.lp: df=lag_feat(df,self.fc,self.lp)
                if self.cr and self.rw: df=roll_feat(df,self.fc,self.rw)
                if self.cd: df=diff_feat(df,self.fc)
                df=df.replace([np.inf,-np.inf],np.nan).bfill().ffill().dropna()
                ex=[self.tc]
                if self.dc: ex.append(self.dc)
                if self.oc and self.oc in df.columns: ex.append(self.oc)
                if tm:
                    df,self.ce=enc_cat_tr(df,ex)
                else:
                    df=enc_cat_inf(df,self.ce,ex)
                ff=[c for c in df.columns if c not in ex]
                if tm:
                    df,ff,self.du=drop_uniq(df,ff,self.ut)
                    self.fo=ff
                else:
                    for c in self.du:
                        if c in df.columns: df=df.drop(columns=[c])
                    mf=[f for f in self.fo if f not in df.columns]
                    if mf:
                        for f in mf: df[f]=0
                    ef=[f for f in df.columns if f not in self.fo and f not in ex]
                    if ef: df=df.drop(columns=ef)
                    ff=self.fo
                X,y=df[ff].values,df[self.tc].values
                if tm:
                    self.sc=MinMaxScaler()
                    Xs=self.sc.fit_transform(X)
                else:
                    if not self.sc: raise ValueError('No scaler')
                    Xs=self.sc.transform(X)
                return seq(Xs,y,self.sl)
            def sv(self,p):
                ensure_dir(p)
                with open(p,'wb') as f: cloudpickle.dump(self,f)
            @staticmethod
            def ld(p):
                with open(p,'rb') as f: return cloudpickle.load(f)

        class DW:
            def __init__(self,tl,tel,id,sc,fc,md):
                self.train_loader=tl;self.test_loader=tel;self.input_dim=id;self.scaler=sc;self.feature_columns=fc;self.metadata=md

        p=argparse.ArgumentParser()
        p.add_argument('--cleaned_data',required=True)
        p.add_argument('--cleaning_metadata',required=True)
        p.add_argument('--target_column',required=True)
        p.add_argument('--feature_columns',default="")
        p.add_argument('--seq_length',type=int,default=10)
        p.add_argument('--test_split',type=float,default=0.2)
        p.add_argument('--batch_size',type=int,default=32)
        p.add_argument('--unique_threshold',type=float,default=0.95)
        p.add_argument('--lag_periods',default="1,2,3,7")
        p.add_argument('--rolling_windows',default="7,14")
        p.add_argument('--create_lags',default="true")
        p.add_argument('--create_rolling_stats',default="true")
        p.add_argument('--create_diff_features',default="true")
        p.add_argument('--processed_data',required=True)
        p.add_argument('--preprocessor',required=True)
        p.add_argument('--engineering_metadata',required=True)
        a=p.parse_args()

        try:
            print('='*80+' TIME SERIES v5.0 '+'='*80)
            el=a.create_lags.lower() in('true','t','yes','y','1')
            er=a.create_rolling_stats.lower() in('true','t','yes','y','1')
            ed=a.create_diff_features.lower() in('true','t','yes','y','1')
            lp=parse_csv(a.lag_periods,int)
            rw=parse_csv(a.rolling_windows,int)
            
            df=pd.read_parquet(a.cleaned_data)
            print('Loaded:',df.shape)
            with open(a.cleaning_metadata) as f: cm=json.load(f)
            
            if a.target_column not in df.columns: raise ValueError('No target:'+a.target_column)
            
            dc=auto_detect_date(df)
            oc=None
            if not dc: dc,oc=detect_timestamp(df)
            if not dc: raise ValueError('No temporal column. Not time series data.')
            
            df[dc]=parse_date(df,dc)
            df=df.sort_values(by=dc).reset_index(drop=True)
            
            if a.feature_columns.strip():
                fc=parse_csv(a.feature_columns,str)
            else:
                ex=[a.target_column,dc]
                if oc: ex.append(oc)
                fc=[c for c in df.columns if c not in ex]
            
            print('Features:',len(fc))
            ish=df.shape
            if el and lp: df=lag_feat(df,fc,lp)
            if er and rw: df=roll_feat(df,fc,rw)
            if ed: df=diff_feat(df,fc)
            print('After eng:',df.shape)
            
            nb=df.isna().sum().sum()
            df=df.replace([np.inf,-np.inf],np.nan).bfill().ffill().dropna()
            print('NaN:',nb,'->',df.isna().sum().sum())
            
            ex=[a.target_column,dc]
            if oc: ex.append(oc)
            df,ei=enc_cat_tr(df,ex)
            print('After enc:',df.shape)
            
            ff=[c for c in df.columns if c not in ex]
            df,ff,du=drop_uniq(df,ff,a.unique_threshold)
            print('After drop uniq:',df.shape,'Dropped:',len(du))
            
            X,y=df[ff].values,df[a.target_column].values
            sc=MinMaxScaler()
            Xs=sc.fit_transform(X)
            Xq,yq=seq(Xs,y,a.seq_length)
            print('Sequences:',Xq.shape)
            
            ts=int(len(Xq)*(1-a.test_split))
            Xtr,ytr=Xq[:ts],yq[:ts]
            Xte,yte=Xq[ts:],yq[ts:]
            
            Xtr=torch.tensor(Xtr,dtype=torch.float32)
            ytr=torch.tensor(ytr,dtype=torch.float32).unsqueeze(1)
            Xte=torch.tensor(Xte,dtype=torch.float32)
            yte=torch.tensor(yte,dtype=torch.float32).unsqueeze(1)
            
            trd=TensorDataset(Xtr,ytr)
            ted=TensorDataset(Xte,yte)
            trl=DataLoader(trd,batch_size=a.batch_size,shuffle=False)
            tel=DataLoader(ted,batch_size=a.batch_size,shuffle=False)
            
            idm=Xtr.shape[2]
            print('Input dim:',idm)
            
            dw=DW(trl,tel,idm,sc,ff,{'sl':a.seq_length,'tc':a.target_column,'dc':dc,'ts':a.test_split,'bs':a.batch_size})
            ensure_dir(a.processed_data)
            with open(a.processed_data,'wb') as f: cloudpickle.dump(dw,f)
            
            pp=TSP()
            pp.fit(df,{'tc':a.target_column,'dc':dc,'oc':oc,'sl':a.seq_length,'lp':lp,'rw':rw,'cl':el,'cr':er,'cd':ed,'ut':a.unique_threshold})
            pp.sc=sc;pp.ce=ei;pp.fo=ff;pp.du=du
            pp.sv(a.preprocessor)
            
            md={'ts':datetime.utcnow().isoformat()+'Z','dc':dc,'tc':a.target_column,'nf':len(ff),'sl':a.seq_length,'split':a.test_split,
                'bs':a.batch_size,'ut':a.unique_threshold,'lp':lp,'rw':rw,'cl':el,'cr':er,'cd':ed,'idm':idm,
                'sh':{'xtr':list(Xtr.shape),'ytr':list(ytr.shape),'xte':list(Xte.shape),'yte':list(yte.shape)},
                'dr':{'st':str(df[dc].min()),'en':str(df[dc].max())},'ish':list(ish),'fsh':list(df.shape),'du':du,'cm':cm}
            ensure_dir(a.engineering_metadata)
            with open(a.engineering_metadata,'w') as f: json.dump(md,f,indent=2)
            
            print('='*80+' SUCCESS '+'='*80)
            print('Train:',Xtr.shape,'Test:',Xte.shape,'Dim:',idm)
        except Exception as e:
            print('ERROR:',str(e),file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --cleaned_data
      - {inputPath: cleaned_data}
      - --cleaning_metadata
      - {inputPath: cleaning_metadata}
      - --target_column
      - {inputValue: target_column}
      - --feature_columns
      - {inputValue: feature_columns}
      - --seq_length
      - {inputValue: seq_length}
      - --test_split
      - {inputValue: test_split}
      - --batch_size
      - {inputValue: batch_size}
      - --unique_threshold
      - {inputValue: unique_threshold}
      - --lag_periods
      - {inputValue: lag_periods}
      - --rolling_windows
      - {inputValue: rolling_windows}
      - --create_lags
      - {inputValue: create_lags}
      - --create_rolling_stats
      - {inputValue: create_rolling_stats}
      - --create_diff_features
      - {inputValue: create_diff_features}
      - --processed_data
      - {outputPath: processed_data}
      - --preprocessor
      - {outputPath: preprocessor}
      - --engineering_metadata
      - {outputPath: engineering_metadata}
