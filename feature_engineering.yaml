name: Time Series Preprocessing v7.3
inputs:
  - {name: cleaned_data, type: Dataset}
  - {name: cleaning_metadata, type: Data}
  - {name: target_column, type: String}
  - {name: feature_columns, type: String, optional: true, default: ""}
  - {name: column_ignore, type: String, optional: true, default: ""}
  - {name: seq_length, type: Integer, optional: true, default: "10"}
  - {name: test_split, type: Float, optional: true, default: "0.2"}
  - {name: batch_size, type: Integer, optional: true, default: "32"}
  - {name: unique_threshold, type: Float, optional: true, default: "0.95"}
  - {name: lag_periods, type: String, optional: true, default: "1,2,3,7"}
  - {name: rolling_windows, type: String, optional: true, default: "7,14"}
  - {name: create_lags, type: String, optional: true, default: "true"}
  - {name: create_rolling_stats, type: String, optional: true, default: "true"}
  - {name: create_diff_features, type: String, optional: true, default: "true"}
  - {name: create_cyclic_features, type: String, optional: true, default: "true"}
  - {name: scaler_type, type: String, optional: true, default: "minmax"}
outputs:
  - {name: processed_data, type: Dataset}
  - {name: preprocessor, type: Data}
  - {name: engineering_metadata, type: Data}
implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v25
    command:
      - python3
      - -u
      - -c
      - |
        import argparse, json, cloudpickle, os, sys, traceback, warnings
        from datetime import datetime
        import pandas as pd, numpy as np, torch
        from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
        from torch.utils.data import TensorDataset, DataLoader

        warnings.filterwarnings('ignore')

        def ensure_dir(p):
            d=os.path.dirname(p)
            if d: os.makedirs(d,exist_ok=True)

        def parse_csv(s,dtype=int):
            return [dtype(x.strip()) for x in s.split(',') if x.strip()] if s.strip() else []

        def auto_detect_date(df):
            print('[AUTO-DETECT] Searching for date columns...')
            kw=['date','time','timestamp','datetime','day','month','year','created','updated','modified','release']
            cands=[]
            for col in df.columns:
                if any(k in col.lower() for k in kw): 
                    cands.append((col,100))
                    print(f'[AUTO-DETECT] Found keyword match: {col}')
            for col in df.columns:
                if col in [c[0] for c in cands] or pd.api.types.is_numeric_dtype(df[col]): continue
                try:
                    s=df[col].dropna().head(100)
                    if len(s)==0: continue
                    p=pd.to_datetime(s,errors='coerce',format='mixed',dayfirst=True)
                    r=p.notna().sum()/len(s)
                    if r>0.8: 
                        cands.append((col,r*50))
                        print(f'[AUTO-DETECT] Parseable date column: {col} ({r:.1%} success)')
                except: pass
            if cands:
                best=max(cands,key=lambda x:x[1])
                print(f'[AUTO-DETECT] Selected: {best[0]}')
                return best[0]
            print('[AUTO-DETECT] No date column found')
            return None

        def detect_timestamp(df):
            print('[TIMESTAMP] Checking for unix timestamps...')
            ranges={'seconds':(946684800,2524608000),'milliseconds':(946684800000,2524608000000),'microseconds':(946684800000000,2524608000000000)}
            kw=['timestamp','epoch','time','unix','ts','created_at','updated_at']
            cands=[]
            for col in df.select_dtypes(include=[np.number]).columns:
                nm=any(k in col.lower() for k in kw)
                s=df[col].dropna()
                if len(s)==0: continue
                mn,mx=s.min(),s.max()
                for u,(rn,rx) in ranges.items():
                    if rn<=mn and mx<=rx:
                        cands.append({'col':col,'unit':u,'pr':100 if nm else 50})
                        print(f'[TIMESTAMP] Found {u} timestamp: {col}')
                        break
            if not cands: 
                print('[TIMESTAMP] No timestamp columns found')
                return None,None
            c=max(cands,key=lambda x:x['pr'])
            col,u=c['col'],c['unit']
            try:
                nc=col+'_datetime'
                df[nc]=pd.to_datetime(df[col],unit='s' if u=='seconds' else 'ms' if u=='milliseconds' else 'us')
                if df[nc].notna().sum()>len(df)*0.95:
                    print(f'[TIMESTAMP] Converted {col} to {nc}')
                    return nc,col
            except: pass
            return None,None

        def parse_date(df,col):
            print(f'[PARSE] Parsing date column: {col}')
            if pd.api.types.is_datetime64_any_dtype(df[col]): 
                print('[PARSE] Already datetime type')
                return df[col]
            strats=[
                ('mixed+dayfirst',lambda x:pd.to_datetime(x,errors='coerce',format='mixed',dayfirst=True)),
                ('mixed',lambda x:pd.to_datetime(x,errors='coerce',format='mixed',dayfirst=False)),
                ('ISO8601',lambda x:pd.to_datetime(x,errors='coerce',format='ISO8601')),
                ('infer',lambda x:pd.to_datetime(x,errors='coerce',infer_datetime_format=True))
            ]
            for nm,st in strats:
                try:
                    p=st(df[col])
                    r=p.notna().sum()/len(df[col])
                    if r>0.95: 
                        print(f'[PARSE] Success with {nm} strategy ({r:.1%})')
                        return p
                except: pass
            raise ValueError(f'Cannot parse date column: {col}')

        def add_cyclic_features(df,dc):
            print('[CYCLIC] Adding temporal cyclic features...')
            dt=pd.to_datetime(df[dc])
            cyc={'hour':(24,'hour_sin','hour_cos'),
                 'dayofweek':(7,'dow_sin','dow_cos'),
                 'day':(31,'day_sin','day_cos'),
                 'month':(12,'month_sin','month_cos'),
                 'quarter':(4,'quarter_sin','quarter_cos')}
            added=[]
            for attr,(period,sn,cn) in cyc.items():
                try:
                    val=getattr(dt.dt,attr)
                    df[sn]=np.sin(2*np.pi*val/period)
                    df[cn]=np.cos(2*np.pi*val/period)
                    added.extend([sn,cn])
                except: pass
            print(f'[CYCLIC] Added {len(added)} features: {added[:6]}...')
            return df,added

        def lag_feat(df,cols,lags):
            nc=[c for c in cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]
            if not nc or not lags: return df,[]
            print(f'[LAG] Creating lag features: {len(nc)} cols x {len(lags)} lags')
            feats={f'{c}_lag{l}':df[c].shift(l) for c in nc for l in lags}
            return pd.concat([df,pd.DataFrame(feats,index=df.index)],axis=1),list(feats.keys())

        def roll_feat(df,cols,wins):
            nc=[c for c in cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]
            if not nc or not wins: return df,[]
            print(f'[ROLLING] Creating rolling features: {len(nc)} cols x {len(wins)} windows')
            feats={}
            for c in nc:
                for w in wins:
                    feats[f'{c}_rmean{w}']=df[c].rolling(w).mean()
                    feats[f'{c}_rstd{w}']=df[c].rolling(w).std()
                    feats[f'{c}_rmin{w}']=df[c].rolling(w).min()
                    feats[f'{c}_rmax{w}']=df[c].rolling(w).max()
            return pd.concat([df,pd.DataFrame(feats,index=df.index)],axis=1),list(feats.keys())

        def diff_feat(df,cols):
            nc=[c for c in cols if c in df.columns and pd.api.types.is_numeric_dtype(df[c])]
            if not nc: return df,[]
            print(f'[DIFF] Creating difference features: {len(nc)} cols')
            # FIX: Proper dict comprehension
            diff_feats={f'{c}_diff':df[c].diff() for c in nc}
            pct_feats={f'{c}_pct':df[c].pct_change() for c in nc}
            feats={**diff_feats,**pct_feats}
            return pd.concat([df,pd.DataFrame(feats,index=df.index)],axis=1),list(feats.keys())

        def enc_cat_tr(df,ex):
            ex=set(ex or [])
            cc=[c for c in df.select_dtypes(include=['object','category']).columns if c not in ex]
            if not cc: return df,{}
            print(f'[ENCODE] Encoding {len(cc)} categorical columns')
            ei={}
            for c in cc:
                nu=df[c].nunique()
                if nu<=10:
                    d=pd.get_dummies(df[c],prefix=c,drop_first=False)
                    # FIX: Convert all dummy columns to float64
                    for dcol in d.columns:
                        d[dcol]=d[dcol].astype(np.float64)
                    df=pd.concat([df.drop(columns=[c]),d],axis=1)
                    ei[c]={'type':'oh','cols':list(d.columns)}
                    print(f'[ENCODE] One-hot: {c} -> {nu} columns')
                else:
                    fm=df[c].value_counts().to_dict()
                    df[c]=df[c].map(fm).fillna(0).astype(np.float64)
                    ei[c]={'type':'freq','map':{str(k):int(v) for k,v in list(fm.items())[:100]}}
                    print(f'[ENCODE] Frequency: {c} ({nu} unique)')
            return df,ei

        def enc_cat_inf(df,ei,ex):
            if not ei: return df
            print(f'[ENCODE-INF] Applying saved encodings to {len(ei)} columns')
            ex=set(ex or [])
            for c,inf in ei.items():
                if c not in df.columns or c in ex: continue
                if inf['type']=='oh':
                    ec=inf['cols']
                    d=pd.get_dummies(df[c],prefix=c,drop_first=False)
                    for e in ec:
                        if e not in d.columns: d[e]=0
                    # FIX: Convert all columns to float64
                    for dcol in d.columns:
                        d[dcol]=d[dcol].astype(np.float64)
                    df=pd.concat([df.drop(columns=[c]),d[ec]],axis=1)
                elif inf['type']=='freq':
                    df[c]=df[c].map(inf['map']).fillna(0).astype(np.float64)
            return df

        def drop_uniq(df,fc,th):
            cd=[]
            ur={}
            print(f'[UNIQUE] Checking uniqueness threshold: {th}')
            for c in fc:
                if c not in df.columns: continue
                up=df[c].nunique()/len(df)
                ur[c]=float(up)
                if up>th:
                    if pd.api.types.is_numeric_dtype(df[c]) and df[c].std()>0: continue
                    cd.append(c)
                    print(f'[UNIQUE] Dropping {c} (uniqueness: {up:.1%})')
            if cd: df=df.drop(columns=cd)
            return df,[c for c in fc if c not in cd],cd,ur

        def seq(d,l,sl):
            if len(d)<=sl:
                raise ValueError(f'Not enough data for sequences: {len(d)} rows, need >{sl}')
            X,y=[],[]
            for i in range(len(d)-sl):
                X.append(d[i:i+sl])
                y.append(l[i+sl])
            return np.array(X),np.array(y)

        def get_scaler(stype):
            scalers={'minmax':MinMaxScaler,'standard':StandardScaler,'robust':RobustScaler}
            return scalers.get(stype.lower(),MinMaxScaler)()

        class TSP:
            def __init__(self):
                self.sc=None;self.fc=[];self.tc=None;self.dc=None;self.oc=None;self.sl=None
                self.lp=[];self.rw=[];self.ce={};self.fo=[];self.cl=True;self.cr=True;self.cd=True
                self.ut=0.95;self.du=[];self.cyc=[];self.ccyc=False;self.st='minmax';self.ur={}
                self.ig=set()
                
            def fit(self,df,cfg):
                self.tc = cfg['tc']
                self.dc = cfg.get('dc')
                self.oc = cfg.get('oc')
                self.sl = cfg['sl']
                self.lp = cfg['lp']
                self.rw = cfg['rw']
                self.cl = cfg['cl']
                self.cr = cfg['cr']
                self.cd = cfg['cd']
                self.ut = cfg.get('ut',0.95)
                self.ccyc = cfg.get('ccyc',False)
                self.st = cfg.get('st','minmax')
            
                # store ignored columns (cfg can pass a list)
                self.ig = set(cfg.get('ig', []) or [])
            
                ex = [self.tc]
                if self.dc: ex.append(self.dc)
                if self.oc: ex.append(self.oc)
            
                # exclude ignored cols from feature columns
                self.fc = [c for c in df.columns if c not in ex and c not in self.ig]
                return self

                
            def tr(self,df,tm=False):
                print(f'[TRANSFORM] Mode: {"TRAIN" if tm else "INFERENCE"}')
                df=df.copy()
                # DROP ignored columns (TRAIN + INFERENCE) — single early enforcement
                if hasattr(self, 'ig') and self.ig:
                    drop_now = [c for c in self.ig if c in df.columns]
                    if drop_now:
                        print(f'[IGNORE] Dropping ignored columns in transform: {drop_now}')
                        df = df.drop(columns=drop_now)
                
                # Auto-detect temporal column
                ddc,dtc=self.dc,None
                if not ddc or ddc not in df.columns:
                    ddc=auto_detect_date(df)
                if not ddc:
                    ddc,dtc=detect_timestamp(df)
                if not ddc:
                    raise ValueError('No temporal column found. Not time series data.')
                
                if tm and ddc!=self.dc:
                    self.dc=ddc
                    if dtc: self.oc=dtc
                
                # Parse and sort
                df[self.dc]=parse_date(df,self.dc)
                df=df.sort_values(by=self.dc).reset_index(drop=True)
                print(f'[SORT] Date range: {df[self.dc].min()} to {df[self.dc].max()}')
                
                # Cyclic features
                if tm and self.ccyc:
                    df,self.cyc=add_cyclic_features(df,self.dc)
                elif not tm and self.cyc:
                    df,_=add_cyclic_features(df,self.dc)
                
                # Feature engineering
                if self.cl and self.lp: 
                    df,lf=lag_feat(df,self.fc,self.lp)
                if self.cr and self.rw: 
                    df,rf=roll_feat(df,self.fc,self.rw)
                if self.cd: 
                    df,df_=diff_feat(df,self.fc)
                
                # Handle inf/nan
                print(f'[CLEAN] Handling inf/nan values')
                df=df.replace([np.inf,-np.inf],np.nan)
                
                # Check which columns have too many NaNs
                nan_cols = []
                for col in df.columns:
                    nan_pct = df[col].isna().sum() / len(df)
                    if nan_pct > 0.5:  # If more than 50% NaN
                        nan_cols.append(col)
                        print(f'[CLEAN] Dropping column {col} ({nan_pct:.1%} NaN)')
                
                if nan_cols:
                    df = df.drop(columns=nan_cols)
                # Forward and backward fill remaining NaNs
                df = df.bfill().ffill()
                
                # Only drop rows if still have NaNs
                before = len(df)
                df = df.dropna()
                if len(df) < before:
                    print(f'[CLEAN] Dropped {before-len(df)} rows with remaining NaN')
                
                # Validation with better error message
                if len(df) <= self.sl:
                    date_range = (df[self.dc].max() - df[self.dc].min()).total_seconds()
                    raise ValueError(
                        f'Insufficient data: {len(df)} rows (need >{self.sl}). '
                        f'Date range is only {date_range:.1f} seconds. '
                        f'Reduce seq_length, lag_periods, and rolling_windows, '
                        f'or fix your timestamp column (currently shows Unix epoch 1970).'
                    )
                
                # Categorical encoding
                ex = [self.tc]
                if self.dc: ex.append(self.dc)
                if self.oc and self.oc in df.columns: ex.append(self.oc)
                # Ensure ignored columns are treated as excluded from features/encoding
                if hasattr(self, 'ig') and self.ig:
                    ex.extend([c for c in self.ig if c not in ex])

                
                # Ensure ignored columns are excluded everywhere
                if hasattr(self, 'ig') and self.ig:
                    ex.extend([c for c in self.ig if c not in ex])

                
                if tm:
                    df,self.ce=enc_cat_tr(df,eXq,yq=seq(Xs,y,self.sl))
                feat_cols = [c for c in ff if c in df.columns]
                if not feat_cols:
                    raise ValueError('No feature columns available after preprocessing.')
                
                # Coerce feature columns to numeric (coerce unparseable -> NaN)
                df[feat_cols] = df[feat_cols].apply(lambda s: pd.to_numeric(s, errors='coerce'))
                
                # Coerce target column too (important)
                df[self.tc] = pd.to_numeric(df[self.tc], errors='coerce')
                
                # Report bad rows where coercion produced NaNs
                nan_row_mask = df[feat_cols + [self.tc]].isna().any(axis=1)
                n_bad = nan_row_mask.sum()
                if n_bad > 0:
                    pct_bad = n_bad / len(df)
                    if pct_bad > 0.05:
                        raise ValueError(
                            f'After coercion, {n_bad} rows ({pct_bad:.1%}) contain non-numeric values in features/target. '
                            'Fix upstream data or adjust preprocessing.'
                        )
                    print(f'[COERCE] Coerced numeric: {n_bad} rows had parse errors — imputing.')
                    df[feat_cols + [self.tc]] = df[feat_cols + [self.tc]].bfill().ffill().fillna(0)
                
                # Final cast
                df[feat_cols] = df[feat_cols].astype(np.float64)
                df[self.tc] = df[self.tc].astype(np.float64)
                
                # Recompute ff
                ff = [c for c in feat_cols if c in df.columns]
                print(f'[COERCE] Features coerced and finalized: {len(ff)} columns')
                # === END STRONG SAFETY ===

                
                # Additional check: ensure all feature columns are numeric
                non_numeric = []
                for col in ff:
                    if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
                        non_numeric.append(col)
                        print(f'[CLEAN] Removing non-numeric column: {col} (type: {df[col].dtype})')
                
                if non_numeric:
                    ff = [c for c in ff if c not in non_numeric]
                    df = df.drop(columns=non_numeric)
                
                print(f'[FEATURES] Selected {len(ff)} numeric features')
                
                # Handle uniqueness
                if tm:
                    df,ff,self.du,self.ur=drop_uniq(df,ff,self.ut)
                    self.fo=ff
                else:
                    # Drop unique columns
                    for c in self.du:
                        if c in df.columns: 
                            df=df.drop(columns=[c])
                    
                    # Align features with training
                    current_feats=set([c for c in df.columns if c not in ex])
                    expected_feats=set(self.fo)
                    
                    # Add missing features (fill with 0)
                    missing=expected_feats-current_feats
                    if missing:
                        print(f'[ALIGN] Adding {len(missing)} missing features')
                        for f in missing: df[f]=0
                    
                    # Remove extra features
                    extra=current_feats-expected_feats
                    if extra:
                        print(f'[ALIGN] Removing {len(extra)} extra features')
                        df=df.drop(columns=list(extra))
                    
                    # Ensure correct order
                    ff=self.fo
                
                # Extract X, y
                X=df[ff].values
                y=df[self.tc].values
                print(f'[FEATURES] X shape: {X.shape}, y shape: {y.shape}')
                
                # Scale
                if tm:
                    self.sc=get_scaler(self.st)
                    Xs=self.sc.fit_transform(X)
                    print(f'[SCALE] Fitted {self.st} scaler')
                else:
                    if not self.sc: raise ValueError('No scaler found')
                    Xs=self.sc.transform(X)
                    print(f'[SCALE] Applied saved scaler')
                
                # Create sequences
                # Create sequences
                Xq,yq = seq(Xs, y, self.sl)
                print(f'[SEQ] Created {len(Xq)} sequences')
                
                # === FINAL GUARANTEE: Ensure Xq and yq are numeric numpy arrays ===
                # Convert to numpy arrays explicitly
                Xq = np.asarray(Xq)
                yq = np.asarray(yq)
                
                # If Xq is object or string dtype - attempt conversion to float
                if not np.issubdtype(Xq.dtype, np.number):
                    try:
                        Xq = Xq.astype(np.float64)  # try a straightforward cast
                        print('[COERCE] Xq converted to float64 successfully.')
                    except Exception as ex:
                        # diagnostic dump to help locate offending columns/values
                        print('[ERROR] Failed to cast Xq to numeric. dtype:', Xq.dtype)
                        try:
                            print('Sample Xq[0]:', Xq[0])
                        except Exception:
                            pass
                        raise TypeError('Non-numeric values present in sequence features (Xq) — check preprocessing and encodings.') from ex
                
                # Same for yq (target)
                if not np.issubdtype(yq.dtype, np.number):
                    try:
                        yq = yq.astype(np.float64)
                        print('[COERCE] yq converted to float64 successfully.')
                    except Exception as ex:
                        print('[ERROR] Failed to cast yq to numeric. dtype:', yq.dtype)
                        try:
                            print('Sample yq[0]:', yq[0])
                        except Exception:
                            pass
                        raise TypeError('Non-numeric values present in sequence targets (yq).') from ex
                
                # Optional: cast to float32 which is ideal for torch
                Xq = Xq.astype(np.float32)
                yq = yq.astype(np.float32)
                
                print('[FINAL] Returning numeric sequences:', Xq.dtype, yq.dtype, 'Xq shape:', Xq.shape, 'yq shape:', yq.shape)
                return Xq, yq

                
            def sv(self,p):
                ensure_dir(p)
                with open(p,'wb') as f: 
                    cloudpickle.dump(self,f)
                print(f'[SAVE] Preprocessor saved: {p}')
                    
            @staticmethod
            def ld(p):
                with open(p,'rb') as f: 
                    return cloudpickle.load(f)

        class DW:
            def __init__(self,tl,tel,id,sc,fc,md):
                self.train_loader=tl;self.test_loader=tel;self.input_dim=id
                self.scaler=sc;self.feature_columns=fc;self.metadata=md

        # Main execution
        p=argparse.ArgumentParser()
        p.add_argument('--cleaned_data',required=True)
        p.add_argument('--cleaning_metadata',required=True)
        p.add_argument('--target_column',required=True)
        p.add_argument('--feature_columns',default="")
        p.add_argument('--seq_length',type=int,default=10)
        p.add_argument('--test_split',type=float,default=0.2)
        p.add_argument('--batch_size',type=int,default=32)
        p.add_argument('--unique_threshold',type=float,default=0.95)
        p.add_argument('--lag_periods',default="1,2,3,7")
        p.add_argument('--rolling_windows',default="7,14")
        p.add_argument('--create_lags',default="true")
        p.add_argument('--create_rolling_stats',default="true")
        p.add_argument('--create_diff_features',default="true")
        p.add_argument('--create_cyclic_features',default="true")
        p.add_argument('--scaler_type',default="minmax")
        p.add_argument('--processed_data',required=True)
        p.add_argument('--preprocessor',required=True)
        p.add_argument('--engineering_metadata',required=True)
        p.add_argument('--column_ignore', default="")
        a=p.parse_args()

        ignore_cols = set(parse_csv(a.column_ignore, str))
        print(f'[IGNORE] Columns to ignore: {list(ignore_cols)}')


        try:
            print('='*80)
            print('TIME SERIES PREPROCESSING v6.0')
            print('='*80)
            
            el=a.create_lags.lower() in('true','t','yes','y','1')
            er=a.create_rolling_stats.lower() in('true','t','yes','y','1')
            ed=a.create_diff_features.lower() in('true','t','yes','y','1')
            ecyc=a.create_cyclic_features.lower() in('true','t','yes','y','1')
            lp=parse_csv(a.lag_periods,int)
            rw=parse_csv(a.rolling_windows,int)
            
            df=pd.read_parquet(a.cleaned_data)
            
            existing_ignore = [c for c in ignore_cols if c in df.columns]
            if existing_ignore:
                print(f'[IGNORE] Dropping columns at load: {existing_ignore}')
                df = df.drop(columns=existing_ignore)
                
            print(f'[LOAD] Data: {df.shape}')
            print(f'[LOAD] Columns: {list(df.columns[:5])}...')
            
            with open(a.cleaning_metadata) as f: cm=json.load(f)
            
            if a.target_column not in df.columns: 
                raise ValueError(f'Target column not found: {a.target_column}')
            print(f'[TARGET] {a.target_column}')
            
            # Initialize preprocessor
            pp=TSP()
            
            # Auto-detect date
            dc=auto_detect_date(df)
            oc=None
            if not dc: 
                dc,oc=detect_timestamp(df)
            if not dc: 
                raise ValueError('No temporal column found')
            
            # Parse and sort
            df[dc]=parse_date(df,dc)
            df=df.sort_values(by=dc).reset_index(drop=True)
            
            # Feature columns
            if a.feature_columns.strip():
                fc = parse_csv(a.feature_columns, str)
            else:
                ex=[a.target_column, dc]
                if oc: ex.append(oc)
                fc = [c for c in df.columns if c not in ex]
            
            # remove ignored columns from fc (so base feature count excludes them)
            if ignore_cols:
                fc = [c for c in fc if c not in ignore_cols]

            
            print(f'[FEATURES] {len(fc)} base features')
            
            # Fit and transform
            pp.fit(df,{
                'tc':a.target_column,'dc':dc,'oc':oc,'sl':a.seq_length,
                'lp':lp,'rw':rw,'cl':el,'cr':er,'cd':ed,'ccyc':ecyc,
                'ut':a.unique_threshold,'st':a.scaler_type,
                'ig': list(ignore_cols)
            })
            
            Xq,yq=pp.tr(df,tm=True)
            
            # Split
            ts=int(len(Xq)*(1-a.test_split))
            Xtr,ytr=Xq[:ts],yq[:ts]
            Xte,yte=Xq[ts:],yq[ts:]
            print(f'[SPLIT] Train: {len(Xtr)}, Test: {len(Xte)}')
            
            # Convert to tensors
            # ensure arrays are numpy float32 (should be, if tr() returned correct types)
            Xtr = np.asarray(Xtr, dtype=np.float32)
            ytr = np.asarray(ytr, dtype=np.float32)
            Xte = np.asarray(Xte, dtype=np.float32)
            yte = np.asarray(yte, dtype=np.float32)
            
            # convert to torch tensors without copying when possible
            Xtr = torch.from_numpy(Xtr)
            ytr = torch.from_numpy(ytr).unsqueeze(1)
            Xte = torch.from_numpy(Xte)
            yte = torch.from_numpy(yte).unsqueeze(1)

            
            trd=TensorDataset(Xtr,ytr)
            ted=TensorDataset(Xte,yte)
            trl=DataLoader(trd,batch_size=a.batch_size,shuffle=False)
            tel=DataLoader(ted,batch_size=a.batch_size,shuffle=False)
            
            idm=Xtr.shape[2]
            print(f'[INPUT] Dimension: {idm}')

            print('[DEBUG] df[ff] dtypes:')
            print(df[ff].dtypes)
            # show top rows for suspect columns (object dtypes)
            obj_cols = [c for c in ff if df[c].dtype == 'object']
            if obj_cols:
                print('[DEBUG] object columns sample values:')
                print(df[obj_cols].head().to_dict())
            
            # Save outputs
            dw=DW(trl,tel,idm,pp.sc,pp.fo,{
                'sl':a.seq_length,'tc':a.target_column,'dc':dc,
                'split':a.test_split,'bs':a.batch_size
            })
            ensure_dir(a.processed_data)
            with open(a.processed_data,'wb') as f: 
                cloudpickle.dump(dw,f)
            print(f'[SAVE] Data wrapper: {a.processed_data}')
            
            pp.sv(a.preprocessor)
            
            md={
                'ts':datetime.utcnow().isoformat()+'Z',
                'version':'6.0',
                'dc':dc,'oc':oc,'tc':a.target_column,
                'nf':len(pp.fo),'sl':a.seq_length,
                'split':a.test_split,'bs':a.batch_size,
                'ut':a.unique_threshold,'scaler':a.scaler_type,
                'lp':lp,'rw':rw,
                'flags':{'lag':el,'roll':er,'diff':ed,'cyclic':ecyc},
                'ignored_columns': list(ignore_cols),
                'idm':idm,
                'shapes':{
                    'train':list(Xtr.shape),
                    'test':list(Xte.shape)
                },
                'date_range':{
                    'start':str(df[dc].min()),
                    'end':str(df[dc].max())
                },
                'dropped_unique':pp.du,
                'uniqueness':pp.ur,
                'cyclic_features':pp.cyc,
                'categorical_encodings':{k:v['type'] for k,v in pp.ce.items()},
                'feature_count':{
                    'base':len(fc),
                    'lag':len(lp)*len(fc) if el else 0,
                    'roll':len(rw)*4*len(fc) if er else 0,
                    'diff':2*len(fc) if ed else 0,
                    'cyclic':len(pp.cyc) if ecyc else 0,
                    'final':len(pp.fo)
                },
                'cleaning_meta':cm
            }
            ensure_dir(a.engineering_metadata)
            with open(a.engineering_metadata,'w') as f: 
                json.dump(md,f,indent=2)
            print(f'[SAVE] Metadata: {a.engineering_metadata}')
            
            print('='*80)
            print('SUCCESS')
            print('='*80)
            print(f'Train: {Xtr.shape} | Test: {Xte.shape}')
            print(f'Features: {len(fc)} -> {idm}')
            print(f'Sequences: {len(Xtr)+len(Xte)}')
            print('='*80)
            
        except Exception as e:
            print('='*80,file=sys.stderr)
            print('ERROR',file=sys.stderr)
            print('='*80,file=sys.stderr)
            print(str(e),file=sys.stderr)
            traceback.print_exc()
            sys.exit(1)
    args:
      - --cleaned_data
      - {inputPath: cleaned_data}
      - --cleaning_metadata
      - {inputPath: cleaning_metadata}
      - --target_column
      - {inputValue: target_column}
      - --feature_columns
      - {inputValue: feature_columns}
      - --seq_length
      - {inputValue: seq_length}
      - --test_split
      - {inputValue: test_split}
      - --batch_size
      - {inputValue: batch_size}
      - --unique_threshold
      - {inputValue: unique_threshold}
      - --lag_periods
      - {inputValue: lag_periods}
      - --rolling_windows
      - {inputValue: rolling_windows}
      - --column_ignore
      - {inputValue: column_ignore}
      - --create_lags
      - {inputValue: create_lags}
      - --create_rolling_stats
      - {inputValue: create_rolling_stats}
      - --create_diff_features
      - {inputValue: create_diff_features}
      - --create_cyclic_features
      - {inputValue: create_cyclic_features}
      - --scaler_type
      - {inputValue: scaler_type}
      - --processed_data
      - {outputPath: processed_data}
      - --preprocessor
      - {outputPath: preprocessor}
      - --engineering_metadata
      - {outputPath: engineering_metadata}
