name: Load API Data v4
description: Fetches JSON data from API endpoint with deterministic pagination and saves as Parquet dataset
inputs:
  - {name: schema_id, type: String, description: 'The schema ID to fetch data from'}
  - {name: token, type: string, description: 'Bearer token for authentication'}
  - {name: url_domain, type: String, description: 'The domain for the API endpoint'}
  - {name: page_size, type: Integer, default: "2000", optional: true, description: 'Number of records per page'}
  - {name: max_pages, type: Integer, default: "0", optional: true, description: 'Maximum pages to fetch (0 = all)'}
outputs:
  - {name: dataset, type: Dataset, description: 'Dataset in Parquet format'}
  - {name: metadata, type: Data, description: 'Metadata about the fetched data'}
implementation:
  container:
    image: python:3.9
    command:
      - sh
      - -c
      - |
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests pandas pyarrow || \
        PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet requests pandas pyarrow --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import sys
        import traceback
        import logging
        from datetime import datetime
        import requests
        import pandas as pd

        logging.basicConfig(
            level=logging.INFO,
            format='%(message)s'
        )
        log = logging.getLogger(__name__)

        def ensure_dir(path):
            d = os.path.dirname(path)
            if d: os.makedirs(d, exist_ok=True)

        def extract_records(body):
            if isinstance(body, list):
                return body
            if isinstance(body, dict):
                for key in ['data', 'items', 'results', 'records', 'content', 'instances']:
                    if key in body and isinstance(body[key], list):
                        return body[key]
                return [body]
            return []

        def discover_page_metadata(body):
            page_size = None
            total_pages = None
            total_instances = None

            if isinstance(body, dict):
                page_size = body.get('pageSize')
                total_pages = body.get('totalPages')
                total_instances = body.get('totalInstances')

                if 'page' in body and isinstance(body['page'], dict):
                    p = body['page']
                    page_size = page_size or p.get('pageSize')
                    total_pages = total_pages or p.get('totalPages')
                    total_instances = total_instances or p.get('totalInstances')

            return page_size, total_pages, total_instances

        def fetch_data_from_api(schema_id, token, url_domain, page_size=2000, max_pages=0):
            log.info('='*80)
            log.info('FETCHING DATA FROM API WITH DETERMINISTIC PAGINATION')
            log.info('='*80)
            log.info(f'Schema ID: {schema_id}')
            log.info(f'Domain: {url_domain}')
            log.info(f'Requested page size: {page_size}')
            log.info(f'Max pages limit: {max_pages} (0 = all)')
            log.info('')
            
            headers = {
                'Content-Type': 'application/json',
                'Authorization': f'Bearer {token}'
            }
            
            payload = {
                'dbType': 'TIDB',
                'ownedOnly': True,
                'filter': {}
            }
            
            log.info('[STEP 1] Making metadata discovery request')
            meta_url = f'https://{url_domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances/list?page=0&size={page_size}&showDBaaSReservedKeywords=true&showPageableMetaData=true'
            
            try:
                resp_meta = requests.post(meta_url, headers=headers, json=payload, timeout=30)
                resp_meta.raise_for_status()
                body_meta = resp_meta.json()
                log.info('[STEP 1] Metadata request successful')
            except requests.exceptions.RequestException as e:
                log.error(f'[STEP 1] Metadata request failed: {e}')
                raise
            
            log.info('[STEP 2] Extracting pagination metadata')
            detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)
            
            log.info(f'[STEP 2]   - pageSize: {detected_page_size}')
            log.info(f'[STEP 2]   - totalPages: {total_pages}')
            log.info(f'[STEP 2]   - totalInstances: {total_instances}')
            
            log.info('[STEP 3] Deciding page size')
            if detected_page_size:
                actual_page_size = min(detected_page_size, page_size)
                log.info(f'[STEP 3] Using server pageSize: {actual_page_size}')
            else:
                inferred_records = extract_records(body_meta)
                actual_page_size = len(inferred_records) if inferred_records else page_size
                log.info(f'[STEP 3] Inferred pageSize from response: {actual_page_size}')
            
            log.info('[STEP 4] Determining total pages')
            if total_pages is None and total_instances:
                total_pages = (total_instances + actual_page_size - 1) // actual_page_size
                log.info(f'[STEP 4] Calculated totalPages from totalInstances: {total_pages}')
            elif total_pages is not None:
                log.info(f'[STEP 4] Using server totalPages: {total_pages}')
            else:
                log.info('[STEP 4] No pagination metadata found')
            
            if max_pages > 0 and total_pages is not None:
                original_total = total_pages
                total_pages = min(total_pages, max_pages)
                if original_total > total_pages:
                    log.info(f'[STEP 4] Limiting from {original_total} to {total_pages} pages (max_pages={max_pages})')
            
            log.info('[STEP 5] Executing pagination strategy')
            all_records = []
            
            if total_pages is None:
                log.info('[STEP 5] Case A: No pagination metadata detected')
                log.info('[STEP 5] Taking metadata response and stopping')
                all_records.extend(extract_records(body_meta))
                log.info(f'[STEP 5] Extracted {len(all_records)} records from single response')
            else:
                log.info(f'[STEP 5] Case B: Pagination detected - fetching {total_pages} pages')
                log.info('')
                
                for page in range(total_pages):
                    page_url = f'https://{url_domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances/list?page={page}&size={actual_page_size}&showDBaaSReservedKeywords=true&showPageableMetaData=true'
                    
                    log.info(f'  [PAGE {page + 1}/{total_pages}] Fetching')
                    
                    try:
                        resp_page = requests.post(page_url, headers=headers, json=payload, timeout=30)
                        resp_page.raise_for_status()
                        body_page = resp_page.json()
                        
                        records = extract_records(body_page)
                        all_records.extend(records)
                        
                        log.info(f'  [PAGE {page + 1}/{total_pages}] Retrieved {len(records)} records (total: {len(all_records)})')
                        
                    except requests.exceptions.RequestException as e:
                        log.error(f'  [PAGE {page + 1}/{total_pages}] Failed: {e}')
                        raise
            
            log.info('')
            log.info('[STEP 6] Pagination complete - deterministic stop condition reached')
            log.info('')
            log.info('='*80)
            log.info('[SUMMARY] Total records fetched: {}'.format(len(all_records)))
            log.info('[SUMMARY] Final dataset size: {} records'.format(len(all_records)))
            log.info('='*80)
            
            if len(all_records) == 0:
                raise ValueError('No data fetched from API')
            
            return all_records

        def convert_to_dataframe(data):
            log.info('='*80)
            log.info('CONVERTING TO DATAFRAME')
            log.info('='*80)
            
            if isinstance(data, list):
                log.info(f'[INFO] Processing list with {len(data)} items')
                if len(data) == 0:
                    raise ValueError('Empty data list')
                
                df = pd.json_normalize(data)
                log.info(f'[INFO] Created DataFrame: {df.shape}')
            elif isinstance(data, dict):
                log.info('[INFO] Processing single dictionary')
                df = pd.json_normalize([data])
                log.info(f'[INFO] Created DataFrame: {df.shape}')
            else:
                raise ValueError(f'Unexpected data type: {type(data)}')
            
            log.info('')
            log.info('[QUALITY] Data quality checks:')
            log.info(f'  - Rows: {len(df):,}')
            log.info(f'  - Columns: {len(df.columns):,}')
            log.info(f'  - Memory: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB')
            
            log.info('')
            log.info('[COLUMNS] Column summary:')
            col_list = list(df.columns[:10])
            suffix = '...' if len(df.columns) > 10 else ''
            log.info(f'  - Columns: {col_list}{suffix}')
            log.info(f'  - Dtypes: {df.dtypes.value_counts().to_dict()}')
            
            null_counts = df.isnull().sum()
            null_cols = null_counts[null_counts > 0]
            if len(null_cols) > 0:
                log.info(f'  - Columns with nulls: {len(null_cols)}')
                for col, count in null_cols.head(5).items():
                    log.info(f'    â€¢ {col}: {count:,} ({count/len(df)*100:.1f}%)')
            else:
                log.info('  - No null values detected')
            
            n_dupes = df.duplicated().sum()
            if n_dupes > 0:
                log.info(f'  - Duplicate rows: {n_dupes:,} ({n_dupes/len(df)*100:.1f}%)')
            else:
                log.info('  - No duplicate rows')
            
            log.info('')
            log.info('[SAMPLE] First 3 rows:')
            log.info(df.head(3).to_string())
            log.info('')
            
            return df

        def save_parquet(df, output_path):
            log.info('='*80)
            log.info('SAVING AS PARQUET')
            log.info('='*80)
            
            ensure_dir(output_path)
            
            try:
                df.to_parquet(output_path, index=False, engine='pyarrow', compression='snappy')
                file_size = os.path.getsize(output_path)
                log.info(f'[SUCCESS] Saved to: {output_path}')
                log.info(f'[SUCCESS] File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
                return True
            except Exception as e:
                log.error(f'[ERROR] Parquet save failed: {e}')
                log.info('[FALLBACK] Trying CSV format')
                
                csv_path = output_path.replace('.parquet', '.csv')
                try:
                    df.to_csv(csv_path, index=False)
                    file_size = os.path.getsize(csv_path)
                    log.info(f'[SUCCESS] Saved as CSV: {csv_path}')
                    log.info(f'[SUCCESS] File size: {file_size:,} bytes ({file_size/1024/1024:.2f} MB)')
                    return False
                except Exception as e2:
                    log.error(f'[ERROR] CSV save also failed: {e2}')
                    raise

        parser = argparse.ArgumentParser(description='Load API Data v2.0')
        parser.add_argument('--schema_id', type=str, required=True)
        parser.add_argument('--token', type=str, required=True)
        parser.add_argument('--url_domain', type=str, required=True)
        parser.add_argument('--page_size', type=int, default=2000)
        parser.add_argument('--max_pages', type=int, default=0)
        parser.add_argument('--dataset', type=str, required=True)
        parser.add_argument('--metadata', type=str, required=True)
        args = parser.parse_args()

        try:
            log.info('='*80)
            log.info('LOAD API DATA v2.0 WITH PAGINATION')
            log.info('='*80)
            log.info(f'Started at: {datetime.utcnow().isoformat()}Z')
            log.info('')
            
            log.info('[AUTH] Reading authentication token')
            with open(args.token, 'r') as f:
                token = f.read().strip()
            log.info('[AUTH] Token loaded successfully')
            log.info('')
            
            data = fetch_data_from_api(
                args.schema_id,
                token,
                args.url_domain,
                args.page_size,
                args.max_pages
            )
            
            df = convert_to_dataframe(data)
            
            parquet_saved = save_parquet(df, args.dataset)
            
            log.info('='*80)
            log.info('CREATING METADATA')
            log.info('='*80)
            
            metadata = {
                'timestamp': datetime.utcnow().isoformat() + 'Z',
                'version': '2.0',
                'schema_id': args.schema_id,
                'url_domain': args.url_domain,
                'format': 'parquet' if parquet_saved else 'csv',
                'shape': list(df.shape),
                'rows': int(df.shape[0]),
                'columns': int(df.shape[1]),
                'column_names': list(df.columns),
                'dtypes': {str(k): str(v) for k, v in df.dtypes.items()},
                'memory_mb': float(df.memory_usage(deep=True).sum() / 1024 / 1024),
                'null_counts': {str(k): int(v) for k, v in df.isnull().sum().items() if v > 0},
                'duplicate_rows': int(df.duplicated().sum()),
                'page_size': args.page_size,
                'max_pages': args.max_pages,
                'output_path': args.dataset
            }
            
            ensure_dir(args.metadata)
            with open(args.metadata, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            log.info(f'[SAVED] Metadata saved to: {args.metadata}')
            log.info('')
            
            log.info('='*80)
            log.info('SUCCESS')
            log.info('='*80)
            log.info(f'Records fetched: {len(df):,}')
            log.info(f'Columns: {len(df.columns):,}')
            log.info(f'Format: {metadata["format"].upper()}')
            log.info(f'Output: {args.dataset}')
            log.info('='*80)
            
        except Exception as e:
            log.error('='*80)
            log.error('ERROR')
            log.error('='*80)
            log.error(f'Failed to load data: {e}')
            traceback.print_exc()
            sys.exit(1)
    args:
      - --schema_id
      - {inputValue: schema_id}
      - --token
      - {inputPath: token}
      - --url_domain
      - {inputValue: url_domain}
      - --page_size
      - {inputValue: page_size}
      - --max_pages
      - {inputValue: max_pages}
      - --dataset
      - {outputPath: dataset}
      - --metadata
      - {outputPath: metadata}
