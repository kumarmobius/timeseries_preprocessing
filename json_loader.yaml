name: Load API Data v3.0
inputs:
  - {name: schema_id, type: String, description: "Schema ID to fetch data from"}
  - {name: token, type: String, description: "Bearer token for authentication"}
  - {name: url_domain, type: String, description: "API domain (no protocol)"}

outputs:
  - {name: dataset, type: Dataset, description: "Complete dataset in Parquet format"}
  - {name: metadata, type: Data, description: "Fetch metadata and pagination details"}

implementation:
  container:
    image: python:3.9-slim
    command:
      - sh
      - -ec
      - |
        pip3 install --quiet requests pandas pyarrow || pip3 install --quiet requests pandas pyarrow --user
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import json
        import os
        import requests
        import pandas as pd
        from datetime import datetime
        import warnings
        warnings.filterwarnings("ignore")

        def ensure_dir(path):
            d = os.path.dirname(path)
            if d:
                os.makedirs(d, exist_ok=True)

        def extract_records(body):
            for key in ("content", "instances", "items", "data", "rows", "records"):
                if isinstance(body, dict) and key in body and isinstance(body[key], list):
                    return body[key]
            if isinstance(body, list):
                return body
            if isinstance(body, dict):
                for v in body.values():
                    if isinstance(v, list):
                        return v
            return []

        def discover_page_metadata(body):
            page_size = None
            total_pages = None
            total_instances = None

            if isinstance(body, dict):
                if "pageSize" in body:
                    page_size = int(body.get("pageSize"))
                if "totalPages" in body:
                    total_pages = int(body.get("totalPages"))
                if "totalInstances" in body:
                    total_instances = int(body.get("totalInstances"))

                if "page" in body and isinstance(body["page"], dict):
                    p = body["page"]
                    page_size = page_size or p.get("pageSize")
                    total_pages = total_pages or p.get("totalPages")

            return page_size, total_pages, total_instances

        parser = argparse.ArgumentParser(description="Load API Data v3.0 (Full Fetch)")
        parser.add_argument("--schema_id", required=True)
        parser.add_argument("--token", required=True)
        parser.add_argument("--url_domain", required=True)
        parser.add_argument("--dataset", required=True)
        parser.add_argument("--metadata", required=True)
        args = parser.parse_args()

        with open(args.token) as f:
            token = f.read().strip()

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {token}"
        }

        payload = {
            "dbType": "TIDB",
            "ownedOnly": False,
            "filter": {}
        }

        BASE_URL = (
            f"https://{args.url_domain}"
            f"/pi-entity-instances-service/v2.0/schemas/{args.schema_id}/instances/list"
        )

        print("DISCOVERING PAGINATION METADATA")
        initial_size = 2000
        meta_url = f"{BASE_URL}?page=0&size={initial_size}&showPageableMetaData=true"

        resp = requests.post(meta_url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        body_meta = resp.json()

        detected_page_size, total_pages, total_instances = discover_page_metadata(body_meta)

        if detected_page_size:
            page_size = min(detected_page_size, initial_size)
        else:
            inferred = extract_records(body_meta)
            page_size = len(inferred) if inferred else initial_size

        if total_pages is None and total_instances:
            total_pages = (int(total_instances) + page_size - 1) // page_size

        print("PAGE SIZE:", page_size)
        print("TOTAL PAGES:", total_pages)
        print("TOTAL INSTANCES:", total_instances)

        collected = []

        if total_pages is None:
            print("NO PAGINATION METADATA â€” USING SINGLE RESPONSE")
            collected.extend(extract_records(body_meta))
        else:
            for page in range(total_pages):
                print(f"FETCHING PAGE {page}")
                page_url = f"{BASE_URL}?page={page}&size={page_size}&showPageableMetaData=true"
                r = requests.post(page_url, headers=headers, json=payload, timeout=60)
                r.raise_for_status()
                collected.extend(extract_records(r.json()))

        if not collected:
            raise RuntimeError("No data fetched from API")

        print("TOTAL RECORDS FETCHED:", len(collected))

        df = pd.json_normalize(collected)

        ensure_dir(args.dataset)
        df.to_parquet(args.dataset, index=False, engine="pyarrow", compression="snappy")

        meta = {
            "schema_id": args.schema_id,
            "url_domain": args.url_domain,
            "records": int(len(df)),
            "columns": int(len(df.columns)),
            "page_size": page_size,
            "total_pages": total_pages,
            "total_instances": total_instances,
            "shape": list(df.shape),
            "timestamp_utc": datetime.utcnow().isoformat() + "Z"
        }

        ensure_dir(args.metadata)
        with open(args.metadata, "w") as f:
            json.dump(meta, f, indent=2)

        print("DATASET SAVED:", args.dataset)
        print("METADATA SAVED:", args.metadata)

  args:
    - --schema_id
    - {inputValue: schema_id}
    - --token
    - {inputPath: token}
    - --url_domain
    - {inputValue: url_domain}
    - --dataset
    - {outputPath: dataset}
    - --metadata
    - {outputPath: metadata}
